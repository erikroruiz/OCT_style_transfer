{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "pix2pix_oct_v3.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/erikroruiz/OCT_style_transfer/blob/main/pix2pix_oct_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xnMOsbqHz61"
      },
      "source": [
        "# pix2pix: Image-to-image translation with a conditional GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITZuApL56Mny"
      },
      "source": [
        "This tutorial demonstrates how to build and train a conditional generative adversarial network (cGAN) called pix2pix that learns a mapping from input images to output images, as described in [Image-to-image translation with conditional adversarial networks](https://arxiv.org/abs/1611.07004) by Isola et al. (2017). pix2pix is not application specific—it can be applied to a wide range of tasks, including synthesizing photos from label maps, generating colorized photos from black and white images, turning Google Maps photos into aerial images, and even transforming sketches into photos.\n",
        "\n",
        "In this example, your network will generate images of building facades using the [CMP Facade Database](http://cmp.felk.cvut.cz/~tylecr1/facade/) provided by the [Center for Machine Perception](http://cmp.felk.cvut.cz/) at the [Czech Technical University in Prague](https://www.cvut.cz/). To keep it short, you will use a [preprocessed copy]((https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/)) of this dataset created by the pix2pix authors.\n",
        "\n",
        "In the pix2pix cGAN, you condition on input images and generate corresponding output images. cGANs were first proposed in [Conditional Generative Adversarial Nets](https://arxiv.org/abs/1411.1784) (Mirza and Osindero, 2014)\n",
        "\n",
        "The architecture of your network will contain:\n",
        "\n",
        "- A generator with a [U-Net]([U-Net](https://arxiv.org/abs/1505.04597))-based architecture.\n",
        "- A discriminator represented by a convolutional PatchGAN classifier (proposed in the [pix2pix paper](https://arxiv.org/abs/1611.07004)).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yo9jD5NoxcRl"
      },
      "source": [
        "### Comprobar el entorno GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tP4EZCzxX2N"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7S1qzqOxgW8"
      },
      "source": [
        "### Comprobar el entorno RAM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXEI1PXExobN"
      },
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1_Y75QXJS6h"
      },
      "source": [
        "## Importamos TensorFlow y otras librerias necesarias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfIk2es3hJEd"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import os\n",
        "import cv2 \n",
        "import time\n",
        "import pathlib\n",
        "import datetime\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from google.colab import drive\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython import display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Psr7SQ01Ocr"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "#Creamos la carpeta origen de Google Drive\n",
        "BASE_FOLDER = '/content/drive/My Drive/TFM/'\n",
        "# Montamos la carpeta en Google drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CbTEt448b4R"
      },
      "source": [
        "# Nombre del experimento\n",
        "EXPERIMENT_NAME = \"pix2pix_119c_119n_paired_bs1\"\n",
        "# Ubicación de las imágenes de train y test\n",
        "PATH_TRAIN = BASE_FOLDER+'datasets/images_pix2pix/119c_119n_paired/train'\n",
        "PATH_TEST = BASE_FOLDER+'datasets/images_pix2pix/119c_119n_paired/test'\n",
        "# The oct training set consist of 364 images\n",
        "BUFFER_SIZE = 119\n",
        "\n",
        "# The batch size of 1 produced better results for the U-Net in the original pix2pix experiment\n",
        "BATCH_SIZE = 1\n",
        "# Each image is 256x256 in size\n",
        "IMG_WIDTH = 256\n",
        "IMG_HEIGHT = 256\n",
        "# Número de steps de entrenamiento (múltiplos de 1000) 200 EPOCHS = 80000 STEPS\n",
        "N_STEPS = 80000\n",
        "\n",
        "\n",
        "\n",
        "# Para el generador U-Net\n",
        "OUTPUT_CHANNELS = 3\n",
        "\n",
        "OPTIMIZER = \"Adam\"\n",
        "\n",
        "# Adam optimizers parameters\n",
        "LEARNING_RATE = 2e-4\n",
        "BETA1=0.5\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYn4MdZnKCey"
      },
      "source": [
        "## Load the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hP6apG0ziaku"
      },
      "source": [
        "### Mostramos una imagen de train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDGPqUIe1WcE"
      },
      "source": [
        "sample_image = tf.io.read_file(str(PATH_TRAIN+'/001.jpg'))\n",
        "sample_image = tf.io.decode_jpeg(sample_image)\n",
        "print(sample_image.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFeoPscHjdbU"
      },
      "source": [
        "plt.figure()\n",
        "plt.imshow(sample_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLKOYm81ijxC"
      },
      "source": [
        "### Mostramos una imagen de test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nSkzWcIiSu-"
      },
      "source": [
        "sample_image = tf.io.read_file(str(PATH_TEST+'/001.jpg'))\n",
        "sample_image = tf.io.decode_jpeg(sample_image)\n",
        "print(sample_image.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTho-WAziXUd"
      },
      "source": [
        "plt.figure()\n",
        "plt.imshow(sample_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fUzsnerj1P3"
      },
      "source": [
        "Cada imagen original tiene un tamaño de `300 x 600` contenindo dos imágenes de  `300 x 300` pixeles:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2A5SU-qxPAqd"
      },
      "source": [
        "Hay que separar la imagen con ruido de la imagen sin ruido. Las dos imágenes resultantes tendrán un tamaño de 300 x 300.\n",
        "\n",
        "Se define una función que separa la imagen de entrada en dos imágenes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aO9ZAGH5K3SY"
      },
      "source": [
        "def load(image_file):\n",
        "  # Read and decode an image file to a uint8 tensor\n",
        "  image = tf.io.read_file(image_file)\n",
        "  image = tf.image.decode_jpeg(image)\n",
        "\n",
        "  # Split each image tensor into two tensors:\n",
        "  # - one with a real building facade image\n",
        "  # - one with an architecture label image \n",
        "  w = tf.shape(image)[1]\n",
        "  w = w // 2\n",
        "  input_image = image[:, w:, :]\n",
        "  real_image = image[:, :w, :]\n",
        "\n",
        "  # Convert both images to float32 tensors\n",
        "  input_image = tf.cast(input_image, tf.float32)\n",
        "  real_image = tf.cast(real_image, tf.float32)\n",
        "\n",
        "  return input_image, real_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuBFEXv7nfLy"
      },
      "source": [
        "inp, re = load(str(PATH_TRAIN+'/050.jpg'))\n",
        "# Casting to int for matplotlib to display the images\n",
        "plt.subplot(121)\n",
        "plt.title('Imagen con ruido')\n",
        "plt.imshow(inp / 255.0)\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.title('Imagen con ruido')\n",
        "plt.imshow(re / 255.0)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVuZQTfI_c-s"
      },
      "source": [
        "As described in the [pix2pix paper](https://arxiv.org/abs/1611.07004), you need to apply random jittering and mirroring to preprocess the training set.\n",
        "\n",
        "Define several functions that:\n",
        "\n",
        "1. Resize each `300 x 300` image to a larger height and width—`286 x 286`.\n",
        "2. Randomly crop it back to `256 x 256`.\n",
        "3. Randomly flip the image horizontally i.e. left to right (random mirroring).\n",
        "4. Normalize the images to the `[-1, 1]` range."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwwYQpu9FzDu"
      },
      "source": [
        "def resize(input_image, real_image, height, width):\n",
        "  input_image = tf.image.resize(input_image, [height, width],\n",
        "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "  real_image = tf.image.resize(real_image, [height, width],\n",
        "                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "\n",
        "  return input_image, real_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yn3IwqhiIszt"
      },
      "source": [
        "def random_crop(input_image, real_image):\n",
        "  stacked_image = tf.stack([input_image, real_image], axis=0)\n",
        "  cropped_image = tf.image.random_crop(\n",
        "      stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n",
        "\n",
        "  return cropped_image[0], cropped_image[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muhR2cgbLKWW"
      },
      "source": [
        "# Normalizing the images to [-1, 1]\n",
        "def normalize(input_image, real_image):\n",
        "  input_image = (input_image / 127.5) - 1\n",
        "  real_image = (real_image / 127.5) - 1\n",
        "\n",
        "  return input_image, real_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVQOjcPVLrUc"
      },
      "source": [
        "@tf.function()\n",
        "def random_jitter(input_image, real_image):\n",
        "  # Resizing to 286x286\n",
        "  input_image, real_image = resize(input_image, real_image, 286, 286)\n",
        "\n",
        "  # Random cropping back to 256x256\n",
        "  input_image, real_image = random_crop(input_image, real_image)\n",
        "\n",
        "  if tf.random.uniform(()) > 0.5:\n",
        "    # Random mirroring\n",
        "    input_image = tf.image.flip_left_right(input_image)\n",
        "    real_image = tf.image.flip_left_right(real_image)\n",
        "\n",
        "  return input_image, real_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E9LGq3WBmsh"
      },
      "source": [
        "Se definen algunas funciones de ayuda para hacer el preprocesamiento de las imágenes de training y test:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyaP4hLJ8b4W"
      },
      "source": [
        "def load_image_train(image_file):\n",
        "  input_image, real_image = load(image_file)\n",
        "  input_image, real_image = random_jitter(input_image, real_image)\n",
        "  input_image, real_image = normalize(input_image, real_image)\n",
        "\n",
        "  return input_image, real_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VB3Z6D_zKSru"
      },
      "source": [
        "def load_image_test(image_file):\n",
        "  input_image, real_image = load(image_file)\n",
        "  input_image, real_image = resize(input_image, real_image,\n",
        "                                   IMG_HEIGHT, IMG_WIDTH)\n",
        "  input_image, real_image = normalize(input_image, real_image)\n",
        "\n",
        "  return input_image, real_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIGN6ouoQxt3"
      },
      "source": [
        "## Generamos los datos de entrada con `tf.data`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQHmYSmk8b4b"
      },
      "source": [
        "# Cargamos las imágenes de training\n",
        "train_dataset = tf.data.Dataset.list_files(str(PATH_TRAIN+'/*.jpg'))\n",
        "# Aplicamos funciones de preprocesado:\n",
        "#  - ramdom jitter: Escalar a 286 x 286 >> cortar aleatoriamente a 256 x 256 >> random mirroring\n",
        "#  - normalizar a [-1, 1]\n",
        "train_dataset = train_dataset.map(load_image_train, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "# Barajamos las imágenes\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
        "# Combinando la imágnes consecutivos del conjunto de datos en lotes.\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
        "\n",
        "# Cargamos las imágenes de test\n",
        "test_dataset = tf.data.Dataset.list_files(str(PATH_TEST+'/*.jpg'))\n",
        "# Aplicamos funciones de preprocesado:\n",
        "#  - Escalar a 256 x 256 \n",
        "#  - normalizar a [-1, 1]\n",
        "test_dataset = test_dataset.map(load_image_test)\n",
        "# Combinando la imágnes consecutivos del conjunto de datos en lotes.\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THY-sZMiQ4UV"
      },
      "source": [
        "## Build the generator\n",
        "\n",
        "The generator of your pix2pix cGAN is a _modified_ [U-Net](https://arxiv.org/abs/1505.04597). A U-Net consists of an encoder (downsampler) and decoder (upsampler). (You can find out more about it in the [Image segmentation](https://www.tensorflow.org/tutorials/images/segmentation) tutorial and on the [U-Net project website](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/).)\n",
        "\n",
        "- Each block in the encoder is: Convolution -> Batch normalization -> Leaky ReLU\n",
        "- Each block in the decoder is: Transposed convolution -> Batch normalization -> Dropout (applied to the first 3 blocks) -> ReLU\n",
        "- There are skip connections between the encoder and decoder (as in the U-Net)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MQPuBCgtldI"
      },
      "source": [
        "Define the downsampler (encoder):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3R09ATE_SH9P"
      },
      "source": [
        "def downsample(filters, size, apply_batchnorm=True):\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  result = tf.keras.Sequential()\n",
        "  result.add(\n",
        "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
        "                             kernel_initializer=initializer, use_bias=False))\n",
        "\n",
        "  if apply_batchnorm:\n",
        "    result.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "  result.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3ksHY2Vd-fT"
      },
      "source": [
        "### Ejemplo. Reduce la dimensionaliad de la imagen de prueba (inp) de 300 x 300 a 150 x 150"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6_uCZCppTh7"
      },
      "source": [
        "# down_model = downsample(3, 4)\n",
        "# down_result = down_model(tf.expand_dims(inp, 0))\n",
        "# print (down_result.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFI_Pa52tjLl"
      },
      "source": [
        "Define the upsampler (decoder):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhgDsHClSQzP"
      },
      "source": [
        "def upsample(filters, size, apply_dropout=False):\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  result = tf.keras.Sequential()\n",
        "  result.add(\n",
        "    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
        "                                    padding='same',\n",
        "                                    kernel_initializer=initializer,\n",
        "                                    use_bias=False))\n",
        "\n",
        "  result.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "  if apply_dropout:\n",
        "      result.add(tf.keras.layers.Dropout(0.5))\n",
        "\n",
        "  result.add(tf.keras.layers.ReLU())\n",
        "\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eX9YEB1geUDs"
      },
      "source": [
        "### Ejemplo. Aumenta la dimensionaliad de la imagen de prueba (inp) de 150 x 150 a 300 x 300"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mz-ahSdsq0Oc"
      },
      "source": [
        "# up_model = upsample(3, 4)\n",
        "# up_result = up_model(down_result)\n",
        "# print (up_result.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueEJyRVrtZ-p"
      },
      "source": [
        "Define the generator with the downsampler and the upsampler:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFPI4Nu-8b4q"
      },
      "source": [
        "def Generator():\n",
        "  inputs = tf.keras.layers.Input(shape=[256, 256, 3])\n",
        "\n",
        "  down_stack = [\n",
        "    downsample(64, 4, apply_batchnorm=False),  # (batch_size, 128, 128, 64)\n",
        "    downsample(128, 4),  # (batch_size, 64, 64, 128)\n",
        "    downsample(256, 4),  # (batch_size, 32, 32, 256)\n",
        "    downsample(512, 4),  # (batch_size, 16, 16, 512)\n",
        "    downsample(512, 4),  # (batch_size, 8, 8, 512)\n",
        "    downsample(512, 4),  # (batch_size, 4, 4, 512)\n",
        "    downsample(512, 4),  # (batch_size, 2, 2, 512)\n",
        "    downsample(512, 4),  # (batch_size, 1, 1, 512)\n",
        "  ]\n",
        "\n",
        "  up_stack = [\n",
        "    upsample(512, 4, apply_dropout=True),  # (batch_size, 2, 2, 1024)\n",
        "    upsample(512, 4, apply_dropout=True),  # (batch_size, 4, 4, 1024)\n",
        "    upsample(512, 4, apply_dropout=True),  # (batch_size, 8, 8, 1024)\n",
        "    upsample(512, 4),  # (batch_size, 16, 16, 1024)\n",
        "    upsample(256, 4),  # (batch_size, 32, 32, 512)\n",
        "    upsample(128, 4),  # (batch_size, 64, 64, 256)\n",
        "    upsample(64, 4),  # (batch_size, 128, 128, 128)\n",
        "  ]\n",
        "\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "  last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n",
        "                                         strides=2,\n",
        "                                         padding='same',\n",
        "                                         kernel_initializer=initializer,\n",
        "                                         activation='tanh')  # (batch_size, 256, 256, 3)\n",
        "\n",
        "  x = inputs\n",
        "\n",
        "  # Downsampling through the model\n",
        "  skips = []\n",
        "  for down in down_stack:\n",
        "    x = down(x)\n",
        "    skips.append(x)\n",
        "\n",
        "  skips = reversed(skips[:-1])\n",
        "\n",
        "  # Upsampling and establishing the skip connections\n",
        "  for up, skip in zip(up_stack, skips):\n",
        "    x = up(x)\n",
        "    x = tf.keras.layers.Concatenate()([x, skip])\n",
        "\n",
        "  x = last(x)\n",
        "\n",
        "  return tf.keras.Model(inputs=inputs, outputs=x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4PKwrcQFYvF"
      },
      "source": [
        "Visualize the generator model architecture:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIbRPFzjmV85"
      },
      "source": [
        "generator = Generator()\n",
        "tf.keras.utils.plot_model(generator, show_shapes=True, dpi=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpDPEQXIAiQO"
      },
      "source": [
        "### Define the generator loss\n",
        "\n",
        "GANs learn a loss that adapts to the data, while cGANs learn a structured loss that penalizes a possible structure that differs from the network output and the target image, as described in the [pix2pix paper](https://arxiv.org/abs/1611.07004).\n",
        "\n",
        "- The generator loss is a sigmoid cross-entropy loss of the generated images and an **array of ones**.\n",
        "- The pix2pix paper also mentions the L1 loss, which is a MAE (mean absolute error) between the generated image and the target image.\n",
        "- This allows the generated image to become structurally similar to the target image.\n",
        "- The formula to calculate the total generator loss is `gan_loss + LAMBDA * l1_loss`, where `LAMBDA = 100`. This value was decided by the authors of the paper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyhxTuvJyIHV"
      },
      "source": [
        "LAMBDA = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1Xbz5OaLj5C"
      },
      "source": [
        "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90BIcCKcDMxz"
      },
      "source": [
        "def generator_loss(disc_generated_output, gen_output, target):\n",
        "  gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
        "\n",
        "  # Mean absolute error\n",
        "  l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
        "\n",
        "  total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
        "\n",
        "  return total_gen_loss, gan_loss, l1_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUgSnmy2nqSP"
      },
      "source": [
        "# # Run the trained model on a few examples from the test set\n",
        "# for inp, tar in test_dataset.take(5):\n",
        "#   generate_images(generator, inp, tar)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSZbDgESHIV6"
      },
      "source": [
        "The training procedure for the generator is as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlB-XMY5Awj9"
      },
      "source": [
        "![Generator Update Image](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/images/gen.png?raw=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTKZfoaoEF22"
      },
      "source": [
        "## Build the discriminator\n",
        "\n",
        "The discriminator in the pix2pix cGAN is a convolutional PatchGAN classifier—it tries to classify if each image _patch_ is real or not real, as described in the [pix2pix paper](https://arxiv.org/abs/1611.07004).\n",
        "\n",
        "- Each block in the discriminator is: Convolution -> Batch normalization -> Leaky ReLU.\n",
        "- The shape of the output after the last layer is `(batch_size, 30, 30, 1)`.\n",
        "- Each `30 x 30` image patch of the output classifies a `70 x 70` portion of the input image.\n",
        "- The discriminator receives 2 inputs: \n",
        "    - The input image and the target image, which it should classify as real.\n",
        "    - The input image and the generated image (the output of the generator), which it should classify as fake.\n",
        "    - Use `tf.concat([inp, tar], axis=-1)` to concatenate these 2 inputs together."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIuTeGL5v45m"
      },
      "source": [
        "Let's define the discriminator:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ll6aNeQx8b4v"
      },
      "source": [
        "def Discriminator():\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  inp = tf.keras.layers.Input(shape=[256, 256, 3], name='input_image')\n",
        "  tar = tf.keras.layers.Input(shape=[256, 256, 3], name='target_image')\n",
        "\n",
        "  x = tf.keras.layers.concatenate([inp, tar])  # (batch_size, 256, 256, channels*2)\n",
        "\n",
        "  down1 = downsample(64, 4, False)(x)  # (batch_size, 128, 128, 64)\n",
        "  down2 = downsample(128, 4)(down1)  # (batch_size, 64, 64, 128)\n",
        "  down3 = downsample(256, 4)(down2)  # (batch_size, 32, 32, 256)\n",
        "\n",
        "  zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3)  # (batch_size, 34, 34, 256)\n",
        "  conv = tf.keras.layers.Conv2D(512, 4, strides=1,\n",
        "                                kernel_initializer=initializer,\n",
        "                                use_bias=False)(zero_pad1)  # (batch_size, 31, 31, 512)\n",
        "\n",
        "  batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
        "\n",
        "  leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
        "\n",
        "  zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)  # (batch_size, 33, 33, 512)\n",
        "\n",
        "  last = tf.keras.layers.Conv2D(1, 4, strides=1,\n",
        "                                kernel_initializer=initializer)(zero_pad2)  # (batch_size, 30, 30, 1)\n",
        "\n",
        "  return tf.keras.Model(inputs=[inp, tar], outputs=last)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdV9yAbBHNkg"
      },
      "source": [
        "Visualize the discriminator model architecture:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHoUui4om-Ev"
      },
      "source": [
        "discriminator = Discriminator()\n",
        "tf.keras.utils.plot_model(discriminator, show_shapes=True, dpi=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOqg1dhUAWoD"
      },
      "source": [
        "### Define the discriminator loss\n",
        "\n",
        "- The `discriminator_loss` function takes 2 inputs: **real images** and **generated images**.\n",
        "- `real_loss` is a sigmoid cross-entropy loss of the **real images** and an **array of ones(since these are the real images)**.\n",
        "- `generated_loss` is a sigmoid cross-entropy loss of the **generated images** and an **array of zeros (since these are the fake images)**.\n",
        "- The `total_loss` is the sum of `real_loss` and `generated_loss`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkMNfBWlT-PV"
      },
      "source": [
        "def discriminator_loss(disc_real_output, disc_generated_output):\n",
        "  real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
        "\n",
        "  generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
        "\n",
        "  total_disc_loss = real_loss + generated_loss\n",
        "\n",
        "  return total_disc_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ede4p2YELFa"
      },
      "source": [
        "The training procedure for the discriminator is shown below.\n",
        "\n",
        "To learn more about the architecture and the hyperparameters you can refer to the [pix2pix paper](https://arxiv.org/abs/1611.07004)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IS9sHa-1BoAF"
      },
      "source": [
        "![Discriminator Update Image](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/images/dis.png?raw=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FMYgY_mPfTi"
      },
      "source": [
        "## Define the optimizers \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbHFNexF0x6O"
      },
      "source": [
        "# Adam optimizer parameters\n",
        "# learning_rate: A Tensor, floating point value, or a schedule that is a tf.keras.optimizers.schedules.LearningRateSchedule, or a callable that takes no arguments and returns the actual value to use, The learning rate. Defaults to 0.001.\n",
        "# beta_1: A float value or a constant float tensor, or a callable that takes no arguments and returns the actual value to use. The exponential decay rate for the 1st moment estimates. Defaults to 0.9.\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(LEARNING_RATE, beta_1=BETA1) # Args: learning rate, beta_1\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(LEARNING_RATE, beta_1=BETA1) # Args: learning rate, beta_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX9RRCYMjmcR"
      },
      "source": [
        "## Define checkpoint-saver"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJnftd5sQsv6"
      },
      "source": [
        "checkpoint_dir = BASE_FOLDER+''+EXPERIMENT_NAME+'/checkpoints'\n",
        "\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep=2)\n",
        "\n",
        "INIT_STEP = 0\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  print('Restoring checkpoint')\n",
        "  checkpoint.restore(ckpt_manager.latest_checkpoint)\n",
        "  INIT_STEP = int(ckpt_manager.latest_checkpoint.split(sep='ckpt-')[-1])*5000\n",
        "  print('Latest checkpoint restored!!')\n",
        "print(INIT_STEP)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rw1fkAczTQYh"
      },
      "source": [
        "## Generate images\n",
        "\n",
        "Write a function to plot some images during training.\n",
        "\n",
        "- Pass images from the test set to the generator.\n",
        "- The generator will then translate the input image into the output.\n",
        "- The last step is to plot the predictions and _voila_!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rb0QQFHF-JfS"
      },
      "source": [
        "Note: The `training=True` is intentional here since\n",
        "you want the batch statistics, while running the model on the test dataset. If you use `training=False`, you get the accumulated statistics learned from the training dataset (which you don't want)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmdVsmvhPxyy"
      },
      "source": [
        "def generate_images(model, test_input, tar):\n",
        "  prediction = model(test_input, training=True)\n",
        "  plt.figure(figsize=(15, 15))\n",
        "\n",
        "  display_list = [test_input[0], tar[0], prediction[0]]\n",
        "  title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
        "\n",
        "  for i in range(3):\n",
        "    plt.subplot(1, 3, i+1)\n",
        "    plt.title(title[i])\n",
        "    # Getting the pixel values in the [0, 1] range to plot.\n",
        "    plt.imshow(display_list[i] * 0.5 + 0.5)\n",
        "    plt.axis('off')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gipsSEoZIG1a"
      },
      "source": [
        "Test the function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Fc4NzT-DgEx"
      },
      "source": [
        "# for example_input, example_target in test_dataset.take(1):\n",
        "#   generate_images(generator, example_input, example_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLKOG55MErD0"
      },
      "source": [
        "## Training\n",
        "\n",
        "- For each example input generates an output.\n",
        "- The discriminator receives the `input_image` and the generated image as the first input. The second input is the `input_image` and the `target_image`.\n",
        "- Next, calculate the generator and the discriminator loss.\n",
        "- Then, calculate the gradients of loss with respect to both the generator and the discriminator variables(inputs) and apply those to the optimizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNNMDBNH12q-"
      },
      "source": [
        "# log_dir=BASE_FOLDER+''+EXPERIMENT_NAME+'/logs/'\n",
        "\n",
        "# summary_writer = tf.summary.create_file_writer(\n",
        "#   log_dir + \"fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBKUV2sKXDbY"
      },
      "source": [
        "# @tf.function\n",
        "def train_step(input_image, target, step):\n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "    gen_output = generator(input_image, training=True)\n",
        "\n",
        "    disc_real_output = discriminator([input_image, target], training=True)\n",
        "    disc_generated_output = discriminator([input_image, gen_output], training=True)\n",
        "\n",
        "    gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)\n",
        "    disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
        "\n",
        "  generator_gradients = gen_tape.gradient(gen_total_loss,\n",
        "                                          generator.trainable_variables)\n",
        "  discriminator_gradients = disc_tape.gradient(disc_loss,\n",
        "                                               discriminator.trainable_variables)\n",
        "\n",
        "  generator_optimizer.apply_gradients(zip(generator_gradients,\n",
        "                                          generator.trainable_variables))\n",
        "  discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n",
        "                                              discriminator.trainable_variables))\n",
        "\n",
        "  # with summary_writer.as_default():\n",
        "  #   tf.summary.scalar('gen_total_loss', gen_total_loss, step=step//1000)\n",
        "  #   tf.summary.scalar('gen_gan_loss', gen_gan_loss, step=step//1000)\n",
        "  #   tf.summary.scalar('gen_l1_loss', gen_l1_loss, step=step//1000)\n",
        "  #   tf.summary.scalar('disc_loss', disc_loss, step=step//1000)\n",
        "  \n",
        "  return np.array([step, gen_total_loss, gen_gan_loss, gen_l1_loss, disc_loss])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82ixzknA5gWq"
      },
      "source": [
        "# Guardar array en disco en formato npy\n",
        "def guardar_array (url_nombre, array):\n",
        "    np.save(url_nombre, array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hx7s-vBHFKdh"
      },
      "source": [
        "The actual training loop. Since this tutorial can run of more than one dataset, and the datasets vary greatly in size the training loop is setup to work in steps instead of epochs.\n",
        "\n",
        "- Iterates over the number of steps.\n",
        "- Every 10 steps print a dot (`.`).\n",
        "- Every 1k steps: clear the display and run `generate_images` to show the progress.\n",
        "- Every 5k steps: save a checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFyPlBWv1B5j"
      },
      "source": [
        "def fit(train_ds, test_ds, steps):\n",
        "  example_input, example_target = next(iter(test_ds.take(1)))\n",
        "  start = time.time()\n",
        "\n",
        "  if (INIT_STEP == 0): # Primera vez que se ejecuta se inicializa el array de métricas\n",
        "    metrics_array = np.zeros((int(N_STEPS/1000),5))\n",
        "    print(\"Entranamiento from scratch\")\n",
        "  else: # Se carga el array de métricas desde disco\n",
        "    print(\"Entrenamiento desde checkpoint\")\n",
        "    metrics_array = np.load(BASE_FOLDER+''+EXPERIMENT_NAME+'/results/metrics.npy')\n",
        "    steps = N_STEPS - INIT_STEP\n",
        "\n",
        "  print(\"Entrenamos \",steps,\" steps desde \", INIT_STEP,\" HASTA \",N_STEPS)\n",
        "\n",
        "  for step, (input_image, target) in train_ds.repeat().take(steps).enumerate():\n",
        "    step = step + INIT_STEP\n",
        "    \n",
        "    if (step) % 1000 == 0:\n",
        "      display.clear_output(wait=True)\n",
        "      print(int(step),' ',N_STEPS)\n",
        "      if step != 0:\n",
        "        print('\\n')\n",
        "        print(f'Time taken for 1000 steps: {time.time()-start:.2f} sec\\n')\n",
        "        # Guardar el array de métricas\n",
        "        indice = int((step / 1000)-1)\n",
        "        print(\"\\nGuardando la fila \",indice,\" en el array de métricas\")\n",
        "        metrica = train_step(input_image, target, step)\n",
        "        metrics_array[indice] = metrica\n",
        "        guardar_array (BASE_FOLDER+''+EXPERIMENT_NAME+'/results/metrics.npy', metrics_array)\n",
        "      else:\n",
        "        metrica = train_step(input_image, target, step)\n",
        "\n",
        "      start = time.time()\n",
        "      generate_images(generator, example_input, example_target)\n",
        "      print(f\"Step: {step//1000}k\")\n",
        "\n",
        "    else:\n",
        "      metrica = train_step(input_image, target, step)\n",
        "\n",
        "    # Training step\n",
        "    if (step+1) % 10 == 0:\n",
        "      print('.', end='', flush=True)\n",
        "\n",
        "    # Save (checkpoint) the model every 5k steps\n",
        "    if (step + 1) % 5000 == 0:\n",
        "      ckpt_save_path = ckpt_manager.save()\n",
        "      print ('\\nSaving checkpoint for step {} at {}'.format(step+1, ckpt_save_path))\n",
        "  \n",
        "  # Guardamos la última métrica\n",
        "  indice = int((N_STEPS / 1000)-1)\n",
        "  print(\"Indice\",indice)\n",
        "  print(\"\\nGuardando la última fila \",indice,\" en el array de métricas\")\n",
        "  metrics_array[indice] = metrica\n",
        "  guardar_array (BASE_FOLDER+''+EXPERIMENT_NAME+'/results/metrics.npy', metrics_array)\n",
        "\n",
        "  # Guardamos el modelo generador G\n",
        "  print(\"Guardamos el modelo resultante\")\n",
        "  generator.save(BASE_FOLDER+'/'+EXPERIMENT_NAME+'/models')  \n",
        "\n",
        "  # Guardamos el array de parámetros\n",
        "  params_array = np.array([EXPERIMENT_NAME,str(BATCH_SIZE),str(N_STEPS),str(N_STEPS/400),str(BUFFER_SIZE),OPTIMIZER])\n",
        "  guardar_array (BASE_FOLDER+''+EXPERIMENT_NAME+'/results/params.npy', params_array)\n",
        "\n",
        "  return metrics_array, metrica\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pe0-8Bzg22ox"
      },
      "source": [
        "Finally, run the training loop:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1zZmKmvOH85"
      },
      "source": [
        "metrics_array, metrica = fit(train_dataset, test_dataset, steps=N_STEPS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSy9hynC-QrZ"
      },
      "source": [
        "metrics_array = np.load(BASE_FOLDER+''+EXPERIMENT_NAME+'/results/metrics.npy')\n",
        "params_array = np.load(BASE_FOLDER+''+EXPERIMENT_NAME+'/results/params.npy')\n",
        "\n",
        "title = \"PIX2PIX\"\n",
        "subtitle = \"Nombre exp.: [\"+params_array[0]+\"] - Batch size: [\"+params_array[1]+\"] - Steps: [\"+params_array[2]+\"] Epochs: [\"+params_array[3]+\"] - Buffer size: [\"+params_array[4]+\"] - Optimizer: [\"+params_array[5]+\"]\"\n",
        "\n",
        "fig = plt.figure(figsize=(16,12))\n",
        "axis = fig.add_subplot(111)\n",
        "\n",
        "plt.title(title+'\\n\\n'+subtitle+'\\n', fontsize=16, pad=10)\n",
        "\n",
        "axis.plot(metrics_array[:,0].astype(int),metrics_array[:,1], marker='o', color=\"#1974D2\" ,label='TOTAL LOSS',linewidth=3, linestyle=\"dashed\") \n",
        "axis.plot(metrics_array[:,0].astype(int),metrics_array[:,2], marker='o', color=\"#FF007F\" ,label='GAN LOSS',linewidth=2) \n",
        "axis.plot(metrics_array[:,0].astype(int),metrics_array[:,3], marker='o', color=\"#FFAA1D\" ,label='L1 LOSS',linewidth=2) \n",
        "axis.plot(metrics_array[:,0].astype(int),metrics_array[:,4], marker='o', color=\"#66FF00\" ,label='DISC LOSS',linewidth=2)\n",
        "axis.legend(loc='best')\n",
        "plt.grid()\n",
        "plt.show() \n",
        "\n",
        "fig.savefig(BASE_FOLDER+''+EXPERIMENT_NAME+'/results/plot.png')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}